---
title: "Markov Chain Monte Carlo II"
subtitle: "PSYC 573"
institute: "University of Southern California"
date: "February 17, 2022"
output: 
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts, "my_style.css"]
    lib_dir: libs
    nature:
      countIncrementalSlides: false
params:
  for_class: false
---
exclude: `r !params$for_class`

# Mid-Semester Feedback

Thanks for sharing your thoughts!
- $n$ = 10

--

What has been useful?
- R command, exercises

--

Struggles/Suggestion?
- Download Rmd from website
- Math concepts
- More R code (especially related to the homework)

---
exclude: `r !params$for_class`
class: clear

Questions?
- Psychology examples
- Posterior predictive check
- Beta likelihood; law of total probability (from conditional to marginal)
- HW 3 ...

--

Changes
- Speed up a bit
- Zoom
- More time in R

---
class: clear

```{r, child = "slide_settings.RMarkdown"}
```

```{r xaringan-panelset, echo = FALSE}
xaringanExtra::use_panelset()
```

```{r xaringan-scribble, echo = FALSE, eval = params$for_class}
xaringanExtra::use_scribble()
```

The original Metropolis (random walk) algorithm allows posterior sampling, without the need to solving the integral

--

However, it is inefficient, especially in high dimension problems (i.e., many parameters)

---
class: inverse, middle, center

# Data Example

Taking notes with a pen or a keyboard?

---
class: clear

### Mueller & Oppenheimer (2014, Psych Science)

.panelset[
.panel[.panel-name[R code]

```{r nt_dat, echo = TRUE}
# Use haven::read_sav() to import SPSS data
nt_dat <- haven::read_sav("https://osf.io/qrs5y/download")
```

]

.panel[.panel-name[Data]

.font70[

0 = laptop, 1 = longhand

```{r nt_dat-show}
library(tidyverse)
nt_dat %>%
    select(condition, wordcount, objectiveZ, openZ) %>%
    group_by(condition) %>%
    sample_n(4) %>%
    knitr::kable()
```

]
]

.panel[.panel-name[Histograms]

```{r hist-wordcount}
ggplot(nt_dat, aes(x = wordcount / 100)) +
    geom_histogram(bins = 10) +
    facet_wrap(~ condition) +
    labs(x = "wordcount / 100")
```

]
]

---
class: clear

> Do people write more or less words when asked to use longhand?

### Normal model

Consider only the laptop group first

```{r wc_laptop}
wc_laptop <- nt_dat$wordcount[nt_dat$condition == 0] / 100
```

$$\text{wc_laptop}_i \sim N(\mu, \sigma^2)$$
Two parameters: $\mu$ (mean), $\sigma^2$ (variance)

---
class: inverse, middle, center

# Gibbs Sampling

---
class: clear

Gibbs sampling is efficient by generating smart proposed values, using **conjugate** or **semiconjugate** priors

Implemented in software like BUGS and JAGS 

--

Useful when:

- Joint posterior is intractable, but the **conditional distributions** are known

---
exclude: `r !params$for_class`

# Conditional Distributions

```{r joint-dens, echo = FALSE, out.width = "90%", fig.asp = 1}
library(plotly)  # for 3D plot
ybar <- 3.096129
s2y <- 1.356451
n <- 31
# Hyperparameters
mu_0 <- 5
sigma2_0 <- 1
kappa_0 <- 1 / 10^2
nu_0 <- 1
# Joint log density
lp_mu_sigma <- function(mu, sigma2) {
    kappa_n <- kappa_0 + n
    mu_n <- (kappa_0 * mu_0 + n * ybar) / kappa_n
    nu_n <- nu_0 + n
    sigma2_n <- (nu_0 * sigma2_0 + (n - 1) * s2y +
        kappa_0 * n / kappa_n * (ybar - mu_0)^2) / nu_n
    log_dens <- -(mu - mu_n)^2 / 2 / sigma2 * kappa_n +
        (-nu_n / 2 - 1) * log(sigma2) +
        (-nu_n * sigma2_n / 2 / sigma2)
    log_dens[is.nan(log_dens)] <- -Inf
    log_dens
}

num_gridpoints <- 51
mu_grid <- seq(2, to = 4, length.out = num_gridpoints)
sigma2_grid <- seq(0.5, to = 4.5, length.out = num_gridpoints)
grid_density <- exp(outer(mu_grid,
    Y = sigma2_grid,
    FUN = lp_mu_sigma
))
plot_ly(
    x = ~sigma2_grid, y = ~mu_grid, z = ~grid_density,
    type = "surface"
)
```

---
class: clear

Another example

.pull-left[

```{r mnorm_persp, fig.asp = 1, out.width = '95%', fig.width = 4.07}
par(mfrow = c(1, 1))
library(mvtnorm)
x_grid <- seq(-3, 3, length.out = 100)
y_grid <- seq(-3, 3, length.out = 100)
xy_grid <- expand.grid(x = x_grid, y = y_grid)
z_grid <- dmvnorm(xy_grid, sigma = matrix(c(1, .7, .7, 1), nrow = 2))
z_grid <- matrix(z_grid, nrow = 100)
persp(x_grid, y_grid, z_grid,
    theta = 0, phi = 40, expand = 0.5,
    col = "lightblue", xlab = "x", ylab = "y", zlab = "density"
)
```

]

.pull-right[

```{r condprob-1, fig.asp = 1, out.width = '95%', fig.width = 4.07}
num_gridpoints <- 51
x_grid <- y_grid <- seq(-3, to = 3, length.out = num_gridpoints)
z_grid <- outer(x_grid,
    Y = y_grid,
    FUN = function(x, y) {
        mnormt::dmnorm(cbind(x, y),
            varcov = matrix(c(1, .7, .7, 1), nrow = 2)
        )
    }
)
cont_layout <- matrix(c(1, 0, 2, 3), 2, 2, byrow = TRUE)
layout(cont_layout, widths = c(0.4, 0.1), heights = c(0.1, 0.4))
par(mar = c(0, 3.5, 0, 0) + 0.1)
curve(dnorm(x, -0.7 * 0.7, sd = sqrt(.3)),
    axes = FALSE, xlim = c(-3, 3),
    ylab = "", col = "red"
)
par(mar = c(3.5, 3.5, 0, 0) + 0.1)
contour(x_grid, y_grid, z_grid,
    xlim = c(-3, 3), ylim = c(-3, 3),
    xlab = "X", ylab = "Y"
)
abline(v = 1, col = "blue")
abline(h = -0.7, col = "red")
xvals <- seq(-3, 3, length.out = 101)
par(mar = c(3.5, 0, 0, 0) + 0.1)
plot(dnorm(xvals, 0.7, sd = sqrt(.3)), xvals,
    type = "l", axes = FALSE,
    ylim = c(-3, 3), col = "blue", xlab = ""
)
```

]

---
class: clear

.font70[

### Conjugate priors for conditional distributions

\begin{align}
    \mu & \sim N(\mu_0, \tau_0^2) \\
    \sigma^2 & \sim \text{Inv-Gamma}(\nu_0 / 2, \nu_0 \sigma^2_0 / 2)
\end{align}

- $\mu_0$: Prior mean, $\tau_0^2$; Prior variance (i.e., uncertainty) of the mean
- $\nu_0$: Prior sample size for the variance; $\sigma^2_0$: Prior expectation of the variance

### Posterior

\begin{align}
    \mu \mid \sigma^2, y & \sim N(\mu_n, \tau_n^2) \\
    \sigma^2 \mid \mu & \sim \text{Inv-Gamma}(\nu_n / 2, \nu_n \sigma^2_n [\mu] / 2)
\end{align}

- $\tau_n^2 = \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}\right)^{-1}$; $\mu_n = \tau_n^2 \left(\frac{\mu_0}{\tau_0^2} + \frac{n \bar y}{\sigma^2}\right)$
- $\nu_n = \nu_0 + n$; $\sigma^2_n (\mu) = \frac{1}{\nu_n} \left[\nu_0 \sigma^2_0 + (n - 1) s^2_y + \sum (\bar y - \mu)^2\right]$

]

---
class: clear

No need for a separate proposal distribution; directly sample the conditional posterior

- Thus, all draws are accepted

--

```{r gibbs-normal}
# Sufficient statistics from data
ybar <- mean(wc_laptop)  # sample mean
s2y <- var(wc_laptop)  # sample variance
n <- length(wc_laptop)  # sample size
# Hyperparameters
mu_0 <- 5
sigma2_0 <- 1
tau2_0 <- 10^2
nu_0 <- 1
# Initialize the Gibbs sampler
set.seed(2120)
num_draws <- 10000
num_warmup <- num_draws / 2
num_chains <- 2
# Initialize a 3-D array (S x # chains x 2 parameters)
post_all_draws <- array(
    dim = c(num_draws, num_chains, 2),
    dimnames = list(NULL, NULL, c("mu", "sigma2"))
)
# Step 1: starting values for sigma2
post_all_draws[1, 1, "sigma2"] <- 1  # for chain 1
post_all_draws[1, 2, "sigma2"] <- 3  # for chain 2
for (s in seq_len(num_draws - 1)) {
    for (j in seq_len(num_chains)) {
        sigma2_s <- post_all_draws[s, j, "sigma2"]
        # Step 2: Sample mu from the conditional posterior
        tau2_n <- 1 / (1 / tau2_0 + n / sigma2_s)
        mu_n <- tau2_n * (mu_0 / tau2_0 + n * ybar / sigma2_s)
        mu_new <- rnorm(1, mean = mu_n, sd = sqrt(tau2_n))
        post_all_draws[s + 1, j, "mu"] <- mu_new
        # Step 3: Sample sigma2 from the conditional posterior
        nu_n <- nu_0 + n  # you could put this line outside the loop
        sigma2_n <- 1 / nu_n *
            (nu_0 * sigma2_0 + (n - 1) * s2y + (ybar - mu_new)^2)
        sigma2_new <- 1 / rgamma(1,
            shape = nu_n / 2,
            rate = nu_n * sigma2_n / 2
        )
        post_all_draws[s + 1, j, "sigma2"] <- sigma2_new
    }
}
# Draws after warm-up
post_draws <- post_all_draws[- (1:num_warmup), , ]
```

### Posterior Summary

2 chains, 10,000 draws each, half warm-ups

$\mu_0$ = `r mu_0`, $\sigma^2_0$ = `r sigma2_0`, $\tau^2_0$ = `r tau2_0`, $\nu_0$ = `r nu_0`

.font70[

```{r trace-th-draws}
library(posterior)
# Convert to `draws_array` object to use the following functions
post_draws_array <- as_draws_array(post_draws)
# Summary (with rhat and ESS)
summarize_draws(post_draws_array) %>%
    knitr::kable()
```

]

The ESS is almost as large as # of draws

